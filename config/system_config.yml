# Main System Configuration (v1.3 compatible)

config_locations:
  personas_dir: "personas" # Relative path to personas dir
  schemas_dir: "schemas"   # Relative path to schemas dir

llm_config:
  providers:
    # Keep your existing provider definitions here (anthropic, openai, etc.)
    # Example:
    anthropic:
      api_base: "https://api.anthropic.com"
      context_length: 200000
      env_var: "ANTHROPIC_API_KEY"
      default_model: "claude-3-5-sonnet-20241022"
      valid_models:
        - "claude-3-7-sonnet-20250219"
        - "claude-3-5-sonnet-20241022"
        - "claude-3-5-haiku-20241022"
        - "claude-3-opus-20240229"
        - "claude-3-sonnet-20240229"
        - "claude-3-haiku-20240307"
      extended_thinking:
        enabled: false
        budget_tokens: 32000
        min_budget_tokens: 1024
        max_budget_tokens: 128000
      litellm_params:
        vertex_project: "{YOUR_GCP_PROJECT_ID}" # Replace with your project ID
        vertex_location: "{YOUR_GCP_REGION}"   # Replace with your region (e.g., us-central1)
        retry: true
        max_retries: 5
        timeout: 30
        rate_limit_policy:
          tokens: 8000
          requests: 50
          period: 60
        backoff:
          initial_delay: 1
          max_delay: 30
          exponential: true
    openai:
      api_base: "https://api.openai.com/v1"
      env_var: "OPENAI_API_KEY"
      default_model: "gpt-4o"
      valid_models:
        - "gpt-4o"
        - "gpt-4o-mini"
        - "gpt-4"
        - "gpt-4-turbo"
        - "o1"
        - "o1-mini"
        - "o3-mini"
      litellm_params:
        retry: true
        max_retries: 3
        timeout: 30
        rate_limit_policy:
          tokens: 4000
          requests: 200
          period: 60
        backoff:
          initial_delay: 1
          max_delay: 20
          exponential: true
    gemini:
      # IMPORTANT: Use the base API endpoint WITHOUT /models
      # LiteLLM will construct this incorrectly, which is why we need this specific format
      api_base: "https://generativelanguage.googleapis.com/v1beta"
      context_length: 32000
      env_var: "GEMINI_API_KEY"
      default_model: "gemini-1.5-pro-latest"
      valid_models:
        - "gemini-2.5-pro-preview-03-25"  # Exact model name that works with curl
      # Set model parameters specific to Gemini
      model_params:
        # Use camelCase format for params as required by Google API
        maxOutputTokens: 8192
        temperature: 0.7
      litellm_params:
        # Skip LiteLLM's Gemini handling with a custom call
        custom_gemini_call: true  # We'll add this flag in our code
        retry: true
        max_retries: 3
        timeout: 60
        backoff:
          initial_delay: 2
          max_delay: 30
          exponential: true
    xai:
      api_base: "https://api.x.ai/v1"  # Official xAI API base URL
      env_var: "XAI_API_KEY"          # Environment variable for API key
      default_model: "grok-1.5-flash" # Defaulting to flash version
      valid_models:
        - "grok-1"                   # Original Grok-1
        - "grok-1.5"                 # Grok 1.5
        - "grok-1.5-flash"           # Faster variant of Grok 1.5
      context_length: 128000         # Adjust based on actual Grok specs
      litellm_params:
        retry: true
        max_retries: 3
        timeout: 45
        rate_limit_policy:
          tokens: 10000             # Adjust based on xAI rate limits
          requests: 50
          period: 60
        backoff:
          initial_delay: 1
          max_delay: 20
          exponential: true

  default_provider: "anthropic"
  default_model: "claude-3-opus-20240229" # Example default

  # Base structure/schema definition for personas
  # Actual definitions are in the personas/ directory
  personas:
    provider: null
    model: null
    temperature: 0.7
    extended_thinking:
      enabled: true
      budget_tokens: 32000
    prompts:
      system: null
      user: null
    execution_plan:
      enabled: false
      steps: []
    model_params:
      max_tokens: 4096
    storage:
      root_dir: "workspaces"
      retention:
        max_age_days: 30
        max_runs: 10
      error_handling:
        ignore_failures: true
        log_level: "ERROR"
    lineage:
      enabled: true
      namespace: "c4h_agents"
      backend:
        type: "file"
        path: "workspaces/lineage"
        url: null
      retention:
        max_age_days: 30
        max_runs: 100
      context:
        include_metrics: true
        include_token_usage: true
        record_timestamps: true

# Define agents config for discovery phase
llm_config:
  default_provider: "anthropic"
  default_model: "claude-3-opus-20240229"
  # Define skills for the orchestration
  skills:
    semantic_iterator:
      module: "c4h_agents.skills.semantic_iterator"
      class: "SemanticIterator"
      description: "Extracts structured information from text in a consistent format"
      method: "execute"
    
    asset_manager:
      module: "c4h_agents.skills.asset_manager"
      class: "AssetManager"
      description: "Manages file creation, modification, and deletion with safety features"
      method: "execute"
    
    semantic_merge:
      module: "c4h_agents.skills.semantic_merge"
      class: "SemanticMerge"
      description: "Intelligently merges changes into existing files"
      method: "execute"
  
  personas:
    # Include the actual persona configurations inline
    discovery_v1:
      provider: "anthropic"
      model: "claude-3-5-sonnet-20241022"
      temperature: 0
      tartxt_config:
        script_path: "c4h_agents/skills/tartxt.py"
        exclusions:
          - '**/node_modules/**'
          - '**/.git/**'
          - '**/__pycache__/**'
          - '**/*.pyc'
          - '**/package-lock.json'
          - '**/dist/**'
          - '**/.DS_Store'
          - '**/README.md'
          - '**/workspaces/**'
          - '**/backup_txt/**'
      prompts:
        system: |
          You are a project discovery agent.
          You analyze project structure and files to understand:
          1. Project organization
          2. File relationships
          3. Code dependencies
          4. Available functionality
    
    solution_designer_v1:
      provider: "anthropic"
      model: "claude-3-7-sonnet-20250219"
      temperature: 1
      extended_thinking:
        enabled: true
        budget_tokens: 32000
      prompts:
        system: |
          You are a code modification solution designer that returns modifications in a clearly structured text format.
          Return your response in the following format, with no additional explanation:

          ===CHANGE_BEGIN===
          FILE: path/to/file
          TYPE: create|modify|delete
          DESCRIPTION: one line description
          DIFF:
          --- a/existing_file.py
          +++ b/existing_file.py
          @@ -1,3 +1,4 @@
          context line
          -removed line
          +added line
          context line
          ===CHANGE_END===

    coder_v1:
      provider: "anthropic"
      model: "claude-3-opus-20240229"
      temperature: 0
      # Add execution plan directly to inline persona definition
      execution_plan:
        enabled: true
        steps:
          - name: "extract_changes"
            type: "skill"
            skill: "semantic_iterator"
            params:
              content: "{{context.input_data.response}}"
              format: "solution_design"
            output_field: "iterator_result"
          
          - name: "process_changes"
            type: "loop"
            iterate_on: "iterator_result.value"
            loop_variable: "current_change"
            body:
              - name: "apply_single_change" 
                type: "skill"
                skill: "asset_manager"
                params:
                  file_path: "{{current_change.file_path}}"
                  content: "{{current_change.content}}"
                  type: "{{current_change.type}}"
                  description: "{{current_change.description}}"
                output_field: "results.changes.{{loop.index}}"
      prompts:
        system: |
          You are an expert code modification agent. Your task is to safely and precisely apply code changes.
          You receive changes in this exact JSON structure:
          {
            "changes": [
              {
                "file_path": "exact path to file",
                "type": "modify",
                "description": "change description",
                "content": "complete file content"
              }
            ]
          }

          Rules:
          1. Always expect input in the above JSON format
          2. If input is a string, parse it as JSON first
          3. Preserve existing functionality unless explicitly told to change it
          4. Maintain code style and formatting
          5. Apply changes exactly as specified
          6. Handle errors gracefully with backups
          7. Validate code after changes

  agents:
    # Define all agent configurations with persona keys
    discovery_phase:
      persona_key: "discovery_v1"  # This links to the inline persona above
    
    solution_design_phase:
      persona_key: "solution_designer_v1"  # This links to inline persona above
    
    coding_phase:
      persona_key: "coder_v1"  # This links to the coder_v1.yml file with execution plan
    
    fallback_coding_phase:
      persona_key: "coder_v1"  # Reusing coder persona for fallback

# Orchestration using the new factory model
orchestration:
  enabled: true
  entry_team: "discovery"
  max_total_teams: 30
  max_recursion_depth: 5
  error_handling:
    retry_teams: true
    max_retries: 2
    log_level: "ERROR"
  teams:
    discovery:
      name: "Discovery Team"
      tasks:
        - name: "discovery_phase" # Unique instance name
          agent_type: "GenericLLMAgent" # Updated to new type format 
          persona_key: "discovery_v1" # References personas/discovery_v1.yml
          description: "Analyze project structure and files" # Optional description
          requires_approval: false
          max_retries: 2
      routing:
        default: "solution"

    solution:
      name: "Solution Design Team"
      tasks:
        - name: "solution_design_phase" # Unique instance name
          agent_type: "GenericLLMAgent" # Updated to new type format
          persona_key: "solution_designer_v1" # References personas/solution_designer_v1.yml
          description: "Create solution design and code changes" # Optional description
          requires_approval: false
          max_retries: 1
      routing:
        # Always go to coder team next - simplified for automatic test execution
        default: "coder"

    coder:
      name: "Coder Team"
      tasks:
        - name: "coding_phase" # Unique instance name
          # Use GenericOrchestratorAgent to use the execution plan
          agent_type: "GenericOrchestratorAgent"
          persona_key: "coder_v1" # References personas/coder_v1.yml with execution_plan
          description: "Implement code changes based on solution design" # Optional description
          requires_approval: false
          max_retries: 1
      routing:
        # End workflow after coder team completes
        default: null

    fallback:
      name: "Fallback Team"
      tasks:
        - name: "fallback_coding_phase" # Unique instance name
          agent_type: "GenericFallbackAgent" # Updated to specific fallback agent
          persona_key: "coder_v1" # Reuse coder persona or have a specific fallback persona
          description: "Implement code changes with conservative approach" # Optional description
          config: # Task-specific override for the persona
            temperature: 0 # Make fallback more conservative
      routing:
        default: null # End workflow after fallback

# Runtime configuration
runtime:
  workflow:
    storage:
      enabled: true
      root_dir: "workspaces/workflows"
      format: "yymmdd_hhmm_{workflow_id}"
      retention:
        max_runs: 10
        max_days: 30
      error_handling:
        ignore_storage_errors: true
        log_level: "ERROR"
  lineage: # Note: Lineage config might also belong under llm_config.personas in v1.3 base structure
    enabled: true
    namespace: "c4h_agents"
    separate_input_output: true
    backend:
      type: "file"
      path: "workspaces/lineage"
    error_handling:
      ignore_failures: true
      log_level: "ERROR"
    context:
      include_metrics: true
      include_token_usage: true
      record_timestamps: true
    retry:
      enabled: true
      max_attempts: 3
      initial_delay: 1
      max_delay: 30
      backoff_factor: 2
      retry_on:
        - "overloaded_error"
        - "rate_limit_error"
        - "timeout_error"

# Backup settings
backup:
  enabled: true
  path: "workspaces/backups"

# Logging configuration
logging:
  level: "debug"
  format: "structured"
  agent_level: "debug"
  providers:
    anthropic:
      level: "debug"
    openai:
      level: "debug"
  truncate:
    prefix_length: 2000
    suffix_length: 1000

# Ensure the file ends properly
