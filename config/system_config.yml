# Main System Configuration (v1.3 compatible)

config_locations:
  personas_dir: "personas" # Relative path to personas dir
  schemas_dir: "schemas"   # Relative path to schemas dir

llm_config:
  providers:
    # Keep your existing provider definitions here (anthropic, openai, etc.)
    # Example:
    anthropic:
      api_base: "https://api.anthropic.com"
      context_length: 200000
      env_var: "ANTHROPIC_API_KEY"
      default_model: "claude-3-5-sonnet-20241022"
      valid_models:
        - "claude-3-7-sonnet-20250219"
        - "claude-3-5-sonnet-20241022"
        - "claude-3-5-haiku-20241022"
        - "claude-3-opus-20240229"
        - "claude-3-sonnet-20240229"
        - "claude-3-haiku-20240307"
      extended_thinking:
        enabled: false
        budget_tokens: 32000
        min_budget_tokens: 1024
        max_budget_tokens: 128000
      litellm_params:
        vertex_project: "{YOUR_GCP_PROJECT_ID}" # Replace with your project ID
        vertex_location: "{YOUR_GCP_REGION}"   # Replace with your region (e.g., us-central1)
        retry: true
        max_retries: 5
        timeout: 30
        rate_limit_policy:
          tokens: 8000
          requests: 50
          period: 60
        backoff:
          initial_delay: 1
          max_delay: 30
          exponential: true
    openai:
      api_base: "https://api.openai.com/v1"
      env_var: "OPENAI_API_KEY"
      default_model: "gpt-4o"
      valid_models:
        - "gpt-4o"
        - "gpt-4o-mini"
        - "gpt-4"
        - "gpt-4-turbo"
        - "o1"
        - "o1-mini"
        - "o3-mini"
      litellm_params:
        retry: true
        max_retries: 3
        timeout: 30
        rate_limit_policy:
          tokens: 4000
          requests: 200
          period: 60
        backoff:
          initial_delay: 1
          max_delay: 20
          exponential: true
    gemini:
      # IMPORTANT: Use the base API endpoint WITHOUT /models
      # LiteLLM will construct this incorrectly, which is why we need this specific format
      api_base: "https://generativelanguage.googleapis.com/v1beta"
      context_length: 32000
      env_var: "GEMINI_API_KEY"
      default_model: "gemini-1.5-pro-latest"
      valid_models:
        - "gemini-2.5-pro-preview-03-25"  # Exact model name that works with curl
      # Set model parameters specific to Gemini
      model_params:
        # Use camelCase format for params as required by Google API
        maxOutputTokens: 8192
        temperature: 0.7
      litellm_params:
        # Skip LiteLLM's Gemini handling with a custom call
        custom_gemini_call: true  # We'll add this flag in our code
        retry: true
        max_retries: 3
        timeout: 60
        backoff:
          initial_delay: 2
          max_delay: 30
          exponential: true
    xai:
      api_base: "https://api.x.ai/v1"  # Official xAI API base URL
      env_var: "XAI_API_KEY"          # Environment variable for API key
      default_model: "grok-1.5-flash" # Defaulting to flash version
      valid_models:
        - "grok-1"                   # Original Grok-1
        - "grok-1.5"                 # Grok 1.5
        - "grok-1.5-flash"           # Faster variant of Grok 1.5
      context_length: 128000         # Adjust based on actual Grok specs
      litellm_params:
        retry: true
        max_retries: 3
        timeout: 45
        rate_limit_policy:
          tokens: 10000             # Adjust based on xAI rate limits
          requests: 50
          period: 60
        backoff:
          initial_delay: 1
          max_delay: 20
          exponential: true

  default_provider: "anthropic"
  default_model: "claude-3-opus-20240229" # Example default

  # Base structure/schema definition for personas
  # Actual definitions are in the personas/ directory
  personas:
    provider: null
    model: null
    temperature: 0.7
    extended_thinking:
      enabled: true
      budget_tokens: 32000
    prompts:
      system: null
      user: null
    execution_plan:
      enabled: false
      steps: []
    model_params:
      max_tokens: 4096
    storage:
      root_dir: "workspaces"
      retention:
        max_age_days: 30
        max_runs: 10
      error_handling:
        ignore_failures: true
        log_level: "ERROR"
    lineage:
      enabled: true
      namespace: "c4h_agents"
      backend:
        type: "file"
        path: "workspaces/lineage"
        url: null
      retention:
        max_age_days: 30
        max_runs: 100
      context:
        include_metrics: true
        include_token_usage: true
        record_timestamps: true

# Define agents config for discovery phase
llm_config:
  default_provider: "anthropic"
  default_model: "claude-3-opus-20240229"
  # Define skills for the orchestration
  skills:
    semantic_iterator:
      module: "c4h_agents.skills.semantic_iterator"
      class: "SemanticIterator"
      description: "Extracts structured information from text in a consistent format"
      method: "execute"
    
    asset_manager:
      module: "c4h_agents.skills.asset_manager"
      class: "AssetManager"
      description: "Manages file creation, modification, and deletion with safety features"
      method: "execute"
    
    semantic_merge:
      module: "c4h_agents.skills.semantic_merge"
      class: "SemanticMerge"
      description: "Intelligently merges changes into existing files"
      method: "execute"
  
  personas:
    discovery_v1:
      provider: "anthropic"
      model: "claude-3-5-sonnet-20241022"
      temperature: 0
      tartxt_config:
        script_path: "/Users/jim/src/apps/c4h_ai_dev/c4h_agents/skills/tartxt.py"
        exclusions:
          - '**/node_modules/**'
          - '**/.git/**'
          - '**/__pycache__/**'
          - '**/*.pyc'
          - '**/package-lock.json'
          - '**/dist/**'
          - '**/.DS_Store'
          - '**/README.md'
          - '**/workspaces/**'
          - '**/backup_txt/**'
      prompts:
        system: |
          You are a project discovery agent.
          You analyze project structure and files to understand:
          1. Project organization
          2. File relationships
          3. Code dependencies
          4. Available functionality
        
        # Add the extract prompt for semantic_slow_extractor
        extract: |
          Extract the {ordinal} change item from the solution design.
          
          IMPORTANT: Return ONLY ONE item at a time.
          
          If there are no more items to extract, respond with "NO_MORE_ITEMS".
          
          Format: {format}
          
          Content:
          {content}
          
          # Instructions
          {instruction}
    
    solution_designer_v1:
      provider: "anthropic"
      model: "claude-3-7-sonnet-20250219"
      temperature: 0.7
      extended_thinking:
        enabled: true
        budget_tokens: 32000
      prompts:
        system: |
          You are a code modification solution designer that returns modifications in a clearly structured text format.
          Return your response in the following format, with no additional explanation:

          ===CHANGE_BEGIN===
          FILE: path/to/file
          TYPE: create|modify|delete
          DESCRIPTION: one line description
          DIFF:
          --- a/existing_file.py
          +++ b/existing_file.py
          @@ -1,3 +1,4 @@
          context line
          -removed line
          +added line
          context line
          ===CHANGE_END===
          
          IMPORTANT: You MUST use the format above. Never return any explanations or analysis outside the ===CHANGE_BEGIN=== and ===CHANGE_END=== blocks.
          Your entire response should only contain one or more of these blocks.
          
        user: |
          Based on the discovery data, create a solution design for adding logging and lineage tracking to the codebase. 
          
          EXTREMELY IMPORTANT: Use the EXACT file paths as shown in the discovery data. Do not create files with paths like 'logging/logger.py' or 'utils/logging.py'. Instead, modify the existing files directly.
          
          The files available for modification are:
          - /Users/jim/src/apps/c4h_ai_dev/tests/test_projects/project1/sample.py
          - /Users/jim/src/apps/c4h_ai_dev/tests/test_projects/project2/main.py
          - /Users/jim/src/apps/c4h_ai_dev/tests/test_projects/project2/utils.py
          
          Use the EXACT format as specified in the system prompt instructions. Your response must consist ONLY of properly formatted change blocks:
          
          ===CHANGE_BEGIN===
          FILE: /exact/path/to/file
          TYPE: create|modify|delete
          DESCRIPTION: one line description
          DIFF:
          [diff content here]
          ===CHANGE_END===
          
          No analysis or explanations outside these blocks.

    coder_v1:
      provider: "anthropic"
      model: "claude-3-opus-20240229"
      temperature: 0
      # Add execution plan directly to inline persona definition
      execution_plan:
        enabled: true
        steps:
          - name: "extract_changes"
            type: "skill"
            skill: "semantic_iterator"
            params:
              content: "{{context.input_data.response}}"
              format: "solution_design"
            output_field: "iterator_result"
          
          - name: "process_changes"
            type: "loop"
            iterate_on: "iterator_result.value"
            loop_variable: "current_change"
            body:
              - name: "apply_single_change" 
                type: "skill"
                skill: "asset_manager"
                params:
                  file_path: "{{current_change.file_path}}"
                  content: "{{current_change.content}}"
                  type: "{{current_change.type}}"
                  description: "{{current_change.description}}"
                output_field: "results.changes.{{loop.index}}"
      prompts:
        system: |
          You are an expert code modification agent. Your task is to safely and precisely apply code changes.
          You receive changes in this exact JSON structure:
          {
            "changes": [
              {
                "file_path": "exact path to file",
                "type": "modify",
                "description": "change description",
                "content": "complete file content"
              }
            ]
          }

          Rules:
          1. Always expect input in the above JSON format
          2. If input is a string, parse it as JSON first
          3. Preserve existing functionality unless explicitly told to change it
          4. Maintain code style and formatting
          5. Apply changes exactly as specified
          6. Handle errors gracefully with backups
          7. Validate code after changes
          
        # Add user prompt to handle changes
        user: |
          I need you to apply these code changes:
          
          {content}
          
          For each change, verify it carefully and apply it exactly as specified.

  agents:
    # Define all agent configurations with persona keys
    discovery_phase:
      persona_key: "discovery_v1"  # This links to the inline persona above
      tartxt_config:
        script_path: "/Users/jim/src/apps/c4h_ai_dev/c4h_agents/skills/tartxt.py"
        input_paths: ["./"]
    
    solution_design_phase:
      persona_key: "solution_designer_v1"  # This links to inline persona above
    
    coding_phase:
      persona_key: "coder_v1"  # This links to the coder_v1.yml file with execution plan
    
    fallback_coding_phase:
      persona_key: "coder_v1"  # Reusing coder persona for fallback
      
    # Skills that can also function as agents need persona configs
    semantic_iterator:
      persona_key: "discovery_v1"  # Use the discovery persona for semantic extraction

# Orchestration using the new factory model
orchestration:
  enabled: true
  entry_team: "discovery"
  max_total_teams: 30
  max_recursion_depth: 5
  error_handling:
    retry_teams: true
    max_retries: 2
    log_level: "ERROR"
  teams:
    discovery:
      name: "Discovery Team"
      tasks:
        - name: "discovery_phase" # Unique instance name
          agent_type: "GenericLLMAgent" # Updated to new type format 
          persona_key: "discovery_v1" # References personas/discovery_v1.yml
          description: "Analyze project structure and files" # Optional description
          requires_approval: false
          max_retries: 2
          tartxt_config:
            script_path: "/Users/jim/src/apps/c4h_ai_dev/c4h_agents/skills/tartxt.py"
            input_paths: ["./tests/test_projects/"]
      routing:
        default: "solution"

    solution:
      name: "Solution Design Team"
      tasks:
        - name: "solution_design_phase" # Unique instance name
          agent_type: "GenericLLMAgent" # Updated to new type format
          persona_key: "solution_designer_v1" # References personas/solution_designer_v1.yml
          description: "Create solution design and code changes" # Optional description
          requires_approval: false
          max_retries: 1
      routing:
        # Always go to coder team next - simplified for automatic test execution
        default: "coder"

    coder:
      name: "Coder Team"
      tasks:
        - name: "coding_phase" # Unique instance name
          # Use GenericOrchestratorAgent to use the execution plan
          agent_type: "GenericOrchestratorAgent"
          persona_key: "coder_v1" # References personas/coder_v1.yml with execution_plan
          description: "Implement code changes based on solution design" # Optional description
          requires_approval: false
          max_retries: 1
          agent_config:
            # Configuration for the semantic_iterator skill
            semantic_iterator:
              persona_key: "discovery_v1"
      routing:
        # End workflow after coder team completes
        default: null

    fallback:
      name: "Fallback Team"
      tasks:
        - name: "fallback_coding_phase" # Unique instance name
          agent_type: "GenericFallbackAgent" # Updated to specific fallback agent
          persona_key: "coder_v1" # Reuse coder persona or have a specific fallback persona
          description: "Implement code changes with conservative approach" # Optional description
          config: # Task-specific override for the persona
            temperature: 0 # Make fallback more conservative
      routing:
        # End workflow after fallback
        default: null # End workflow after fallback

    # Example Fan-Out Pattern - can be uncommented and customized as needed
    # fan_out_example:
    #   name: "Fan-Out Example Team"
    #   tasks:
    #     - name: "prepare_for_parallel"
    #       agent_type: "GenericLLMAgent"
    #       persona_key: "discovery_v1"
    #       description: "Prepare data for parallel processing"
    #   routing:
    #     rules:
    #       - condition:
    #           task: "prepare_for_parallel"
    #           status: "success"
    #         # Fan-out by specifying multiple target teams
    #         next_team: ["parallel_team_a", "parallel_team_b", "parallel_team_c"]
    #         context_updates:
    #           fan_out_metadata:
    #             source_team: "fan_out_example"
    #             timestamp: "{{runtime.timestamp}}"
    #     default: null
    
    # # Example Parallel Teams (targets for fan-out)
    # parallel_team_a:
    #   name: "Parallel Team A"
    #   tasks:
    #     - name: "process_part_a"
    #       agent_type: "GenericLLMAgent"
    #       persona_key: "discovery_v1"
    #       description: "Process part A of the task"
    #   routing:
    #     # Route to the concentrator for aggregation
    #     default: "concentration_team"
    
    # parallel_team_b:
    #   name: "Parallel Team B"
    #   tasks:
    #     - name: "process_part_b"
    #       agent_type: "GenericLLMAgent"
    #       persona_key: "discovery_v1"
    #       description: "Process part B of the task"
    #   routing:
    #     # Route to the concentrator for aggregation
    #     default: "concentration_team"
    
    # parallel_team_c:
    #   name: "Parallel Team C"
    #   tasks:
    #     - name: "process_part_c"
    #       agent_type: "GenericLLMAgent"
    #       persona_key: "discovery_v1"
    #       description: "Process part C of the task"
    #   routing:
    #     # Route to the concentrator for aggregation
    #     default: "concentration_team"
    
    # # Example Concentrator Pattern
    # concentration_team:
    #   name: "Concentrator Team"
    #   type: "concentrator"  # Special team type for fan-in/concentration
    #   # List of teams whose results should be collected
    #   source_teams: ["parallel_team_a", "parallel_team_b", "parallel_team_c"]
    #   # Completion condition to determine when aggregation should occur
    #   completion_condition:
    #     type: "count"  # Wait for a specific number of results
    #     value: 3       # Require all 3 teams to report back
    #   # How to combine results from the source teams
    #   aggregation_strategy:
    #     type: "list"  # Collect results in a list
    #     output_field: "aggregated_parallel_results"  # Name of field in context to store results
    #   routing:
    #     rules:
    #       - condition:
    #           context_field: "aggregated_parallel_results"
    #           operator: "exists"  # Check if aggregation was successful
    #         next_team: "final_processing_team"  # Continue to next step with aggregated results
    #     default: "fallback"  # In case of aggregation failure

# Runtime configuration
runtime:
  workflow:
    storage:
      enabled: true
      root_dir: "workspaces/workflows"
      format: "yymmdd_hhmm_{workflow_id}"
      retention:
        max_runs: 10
        max_days: 30
      error_handling:
        ignore_storage_errors: true
        log_level: "ERROR"
  lineage: # Note: Lineage config might also belong under llm_config.personas in v1.3 base structure
    enabled: true
    namespace: "c4h_agents"
    separate_input_output: true
    backend:
      type: "file"
      path: "workspaces/lineage"
    error_handling:
      ignore_failures: true
      log_level: "ERROR"
    context:
      include_metrics: true
      include_token_usage: true
      record_timestamps: true
    retry:
      enabled: true
      max_attempts: 3
      initial_delay: 1
      max_delay: 30
      backoff_factor: 2
      retry_on:
        - "overloaded_error"
        - "rate_limit_error"
        - "timeout_error"

# Backup settings
backup:
  enabled: true
  path: "workspaces/backups"

# Logging configuration
logging:
  level: "debug"
  format: "structured"
  agent_level: "debug"
  providers:
    anthropic:
      level: "debug"
    openai:
      level: "debug"
  truncate:
    prefix_length: 2000
    suffix_length: 1000

# Ensure the file ends properly